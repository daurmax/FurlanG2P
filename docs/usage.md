# Usage Guide

This guide focuses on the hybrid G2P workflows exposed by the current CLI and
service APIs.

## Quick command map

```bash
furlang2p normalize "CJASE 1964 kg"
furlang2p g2p "Cjase"
furlang2p ipa "ìsule glace"
furlang2p phonemize-csv --in metadata.csv --out out.csv
furlang2p lexicon --help
furlang2p evaluate --help
furlang2p coverage --help
```

## Lexicon building workflow

### 1) Prepare source files

Supported input sources:
- WikiPron TSV (`--source-type wikipron`)
- Generic TSV (`--source-type tsv`)
- Manual TSV-style ingestion (`--source-type manual`)

Typical TSV rows:

```tsv
lemma	ipa	dialect	source	confidence	frequency	alternatives
cjase	ˈcaze	central	manual	1.0	120	[]
```

Minimal TSV is also accepted:

```tsv
lemma	ipa
cjase	ˈcaze
```

### 2) Build merged lexicon

```bash
furlang2p lexicon build data/source.tsv \
  --output data/lexicon.jsonl \
  --format jsonl \
  --source-type tsv \
  --dialect central
```

### 3) Inspect statistics

```bash
furlang2p lexicon info data/lexicon.jsonl
furlang2p lexicon info data/lexicon.jsonl --json
```

### 4) Export alternate formats

```bash
furlang2p lexicon export data/lexicon.jsonl data/lexicon.tsv --format tsv
furlang2p lexicon export data/lexicon.jsonl data/lexicon-simple.tsv --format tsv-simple
```

### 5) Validate integrity

```bash
furlang2p lexicon validate data/lexicon.jsonl
furlang2p lexicon validate data/lexicon.jsonl --strict --json
```

## Evaluation workflow

### 1) Prepare gold TSV

Accepted rows:
- `word<TAB>ipa`
- `word<TAB>ipa<TAB>dialect`

Example:

```tsv
cjase	ˈcaze	central
aghe	ˈage
```

### 2) Run evaluation

```bash
furlang2p evaluate gold.tsv
furlang2p evaluate gold.tsv --verbose
furlang2p evaluate gold.tsv --format json --output eval.json
furlang2p evaluate gold.tsv --dialect western --lexicon data/lexicon.jsonl
```

### 3) Interpret results

Key metrics:
- `WER`: word-level mismatch rate.
- `PER`: phoneme edit distance normalized by gold phoneme count.
- `Stress accuracy`: primary stress position agreement where gold stress exists.

Notes:
- `--verbose` prints per-word error lines to stdout.
- `--output` always writes detailed results (including `details`) to file.

## Coverage analysis workflow

### 1) Prepare word list

One word per line:

```text
cjase
aghe
123
```

### 2) Run coverage

```bash
furlang2p coverage words.txt
furlang2p coverage words.txt --show-oov
furlang2p coverage words.txt --format json --output coverage.json
furlang2p coverage words.txt --dialect carnic --lexicon data/lexicon.jsonl
```

### 3) Interpret classes

- `lexicon`: word found in lexicon.
- `rule_only`: generated by rules after lexicon miss.
- `oov`: not handled by lexicon or rules.

Coverage is `lexicon + rule_only` over total words.

## Dialect selection

Dialect can be set globally or per request.

CLI:
- `evaluate --dialect ...` overrides dialect used for predictions.
- `coverage --dialect ...` conditions both lookup and rule classification.
- Gold TSV dialect column is used automatically when `evaluate` has no explicit
  `--dialect`.

Python API:

```python
from furlan_g2p.services import PipelineService

pipe = PipelineService(default_dialect="central")
norm, phonemes = pipe.process_text("Cjase")  # uses default dialect
norm2, phonemes2 = pipe.process_text("Cjase", dialect="western")  # override
```

## Optional ML support

Install variants:

```bash
pip install furlang2p
pip install "furlang2p[ml]"
```

Current behavior:
- Base install works without ML dependencies.
- `furlan_g2p.ml` always exposes `IExceptionModel` and `NullExceptionModel`.
- In current runtime, `NullExceptionModel` means no ML prediction is applied
  unless a concrete model is integrated.

## Pipeline API examples

```python
from furlan_g2p.services import PipelineService

pipe = PipelineService()
norm, phonemes = pipe.process_text("Cjase")
print(norm)      # cjase
print(phonemes)  # ['ˈc', 'a', 'z', 'e']
```

Batch CSV processing:

```python
pipe.process_csv("metadata.csv", "out.csv")
```

With dialect column (zero-based index):

```python
pipe.process_csv("metadata.csv", "out.csv", dialect_column=2)
```

## Configurable normalizer/tokenizer

```python
from furlan_g2p.config import load_normalizer_config, load_tokenizer_config
from furlan_g2p.normalization import Normalizer
from furlan_g2p.tokenization import Tokenizer

norm_cfg = load_normalizer_config("norm_rules.yml")
tok_cfg = load_tokenizer_config("tok_rules.yml")

print(Normalizer(norm_cfg).normalize("1964 kg"))
print(Tokenizer(tok_cfg).split_sentences("Al è rivât il Sig. Bepo. O ven?"))
```
